{
  "meta": [
    {
      "job_title": "Scala Developer",
      "job_category": "Software Engineering",
      "job_class": "Developer",
      "sector": "Software Engineering",
      "remote": true
    }
  ],
  "skills": [
    {
      "skill": "Scala",
      "experience": 7
    },
    {
      "skill": "Python",
      "experience": 7
    },
    {
      "skill": "Apache Spark",
      "experience": 7
    },
    {
      "skill": "SQL",
      "experience": 7
    },
    {
      "skill": "Hadoop",
      "experience": 7
    },
    {
      "skill": "Kafka",
      "experience": 7
    },
    {
      "skill": "ETL",
      "experience": 7
    },
    {
      "skill": "Data Engineering",
      "experience": 7
    },
    {
      "skill": "Git",
      "experience": 7
    },
    {
      "skill": "Shell Scripting",
      "experience": 7
    },
    {
      "skill": "Java",
      "experience": 7
    },
    {
      "skill": "Debugging",
      "experience": 7
    },
    {
      "skill": "Communication Skills",
      "experience": 7
    },
    {
      "skill": "Agile",
      "experience": 7
    },
    {
      "skill": "Leadership",
      "experience": 7
    },
    {
      "skill": "Self-Motivation",
      "experience": 7
    },
    {
      "skill": "Technical Leadership",
      "experience": 7
    },
    {
      "skill": "Learning Agility",
      "experience": 7
    },
    {
      "skill": "Problem-Solving",
      "experience": 7
    },
    {
      "skill": "Process Improvement",
      "experience": 7
    },
    {
      "skill": "Incident Management",
      "experience": 7
    }
  ],
  "duties_responsibilities": [
    {
      "duty": "Develop scalable data pipelines using Scala, Python, pyspark, Hadoop, Apache Spark, Spark SQL, Kafka, Nifi, ETL and Incremental Data Load with Big data technologies."
    },
    {
      "duty": "Work with databases like Oracle, Netezza, and have strong SQL knowledge."
    },
    {
      "duty": "Develop scalable streaming solutions based on Hadoop/Big Data stack such as HDFS, Ozone, Hive/Impala, Apache Spark, and Kafka."
    },
    {
      "duty": "Implement multiple end-to-end DW projects in a Big Data environment."
    },
    {
      "duty": "Build data pipelines through Git, Shell, Spark with Java/Python on Hadoop or Object storage (S3/CEPH)."
    },
    {
      "duty": "Use strong analytical skills for debugging production issues, providing root cause, and implementing mitigation plans."
    },
    {
      "duty": "Demonstrate strong verbal and written communication skills, collaboration skills, and organizational skills."
    },
    {
      "duty": "Be flexible to stretch and work in India time to coordinate with the India team."
    },
    {
      "duty": "Develop experience in Python-based AI/ML workloads on Hadoop."
    },
    {
      "duty": "Work on Nifi, APIs, and Unix Environment."
    },
    {
      "duty": "Work in Agile teams, multi-task across multiple projects, interface with external/internal resources, and provide technical leadership to junior team members."
    },
    {
      "duty": "Function under pressure in an independent environment, with a high degree of initiative, self-motivation, and the ability to quickly learn and implement new technologies."
    },
    {
      "duty": "Contribute ideas to ensure required standards and processes are in place and look for opportunities to enhance standards and improve process efficiency."
    },
    {
      "duty": "Perform assigned tasks and production incidents independently."
    },
    {
      "duty": "Deliver results and demonstrate willingness to get the job done."
    }
  ],
  "subjective_elements": [
    {
      "soldier_general": 75,
      "student_teacher": 100,
      "introvert_extrovert": 50
    }
  ],
  "education_certification": [
    {
      "credential": "Bachelor's degree in Computer Science or related field (Master's preferred)"
    }
  ]
}